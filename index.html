<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>EcoSort</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <style>
    :root {
      --green-main: #166534;     
      --green-soft: #15803d;
      --gray-bg:   #f5f5f5;       
      --gray-border: #e5e5e5;
      --text-muted: #4b5563;
      --max-width: 1000px; 
    }

    * {
      box-sizing: border-box;
      scroll-behavior: smooth;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text",
                   system-ui, -system-ui, "Segoe UI", sans-serif;
      background: #ffffff;
      color: var(--green-main);
    }

    a {
      color: inherit;
      text-decoration: none;
    }

    header {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      height: 56px;
      background: rgba(255, 255, 255, 0.9);
      backdrop-filter: blur(10px);
      border-bottom: 1px solid rgba(229, 231, 235, 0.8);
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 0 24px;
      z-index: 20;
    }

    .logo {
      font-weight: 600;
      letter-spacing: 0.04em;
      font-size: 14px;
      text-transform: none;
    }

    nav {
      display: flex;
      gap: 18px;
      font-size: 13px;
    }

    nav a {
      padding: 4px 8px;
      border-radius: 999px;
      color: var(--text-muted);
    }

    nav a:hover {
      background: #f3f4f6;
      color: var(--green-main);
    }

    main {
      padding-top: 72px;
    }

    section {
      min-height: 70vh;
      padding: 40px 20px 60px;
      display: flex;
      justify-content: center;
    }

    .section-inner {
      width: 100%;
      max-width: var(--max-width);
    }

    .section-label {
      font-size: 11px;
      letter-spacing: 0.16em;
      text-transform: uppercase;
      color: var(--text-muted);
      margin-bottom: 8px;
    }

    h1, h2 {
      margin: 0 0 16px;
      font-weight: 600;
      letter-spacing: 0.02em;
    }

    h1 {
      font-size: clamp(32px, 5vw, 48px);
    }

    h2 {
      font-size: 24px;
    }

    p {
      margin: 0 0 12px;
      line-height: 1.6;
      color: var(--text-muted);
      font-size: 15px;
    }

    .card {
      background: var(--gray-bg);
      border-radius: 16px;
      border: 1px solid var(--gray-border);
      padding: 28px 32px;
    }

    .intro-card {
      display: grid;
      grid-template-columns: minmax(0, 2.2fr) minmax(0, 1.8fr); 
      gap: 36px;
      align-items: stretch;
      margin-bottom: 32px;
    }

    .intro-card .intro-text {
      display: flex;
      flex-direction: column;
      gap: 8px;
      text-align: justify;
    }

    .intro-card .intro-image {
      width: 100%;
      height: 100%;
      min-height: 220px;
      border-radius: 12px;
      overflow: hidden;
      border: 1px solid #d1d5db;
      background: #f8fafc;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 10px;
    }

  
    .intro-card .intro-image img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      display: block;
      border-radius: 12px; 
    }


    .intro-card .intro-image.contain img {
      object-fit: contain;
      padding: 6px;
      background: #ffffff;
    }

    .intro-image.extra-pad {
      padding: 20px !important;   
    }

    @media (max-width: 800px) {
      .intro-card {
        grid-template-columns: 1fr;
      }
    }



    .two-column {
      display: grid;
      grid-template-columns: minmax(0, 3fr) minmax(0, 2fr);
      gap: 24px;
    }

    @media (max-width: 800px) {
      .two-column {
        grid-template-columns: 1fr;
      }
    }

    /* Hero section */
    .hero {
      min-height: 80vh;
      display: flex;
      align-items: center;
      justify-content: center;
      padding-top: 80px;
    }

    .hero-title {
      font-size: clamp(40px, 7vw, 60px);
    }

    .hero-tagline {
      font-size: 15px;
      max-width: 480px;
      color: var(--text-muted);
      margin-top: 12px;
    }

    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-top: 20px;
      font-size: 12px;
      color: var(--text-muted);
    }

    .pill {
      padding: 4px 10px;
      border-radius: 999px;
      border: 1px solid var(--gray-border);
      background: #ffffff;
    }

    /* Scroll arrow */
    .scroll-indicator {
      margin-top: 32px;
      display: inline-flex;
      flex-direction: column;
      align-items: center;
      gap: 4px;
      font-size: 11px;
      color: var(--text-muted);
      cursor: pointer;
    }

    .arrow-down {
      font-size: 18px;
      animation: bounce 1.5s infinite;
    }

    @keyframes bounce {
      0%, 100% { transform: translateY(0); }
      50% { transform: translateY(6px); }
    }

    .image-placeholder {
      width: 100%;
      min-height: 180px;
      border-radius: 16px;
      background: linear-gradient(135deg, #f3f4f6, #e5e7eb);
      border: 1px dashed #d1d5db;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 13px;
      color: var(--text-muted);
      text-align: center;
      padding: 16px;
    }

    ul {
      padding-left: 18px;
      margin: 0 0 12px;
      color: var(--text-muted);
      font-size: 15px;
    }

    li {
      margin-bottom: 6px;
    }

    footer {
      padding: 24px 20px 40px;
      text-align: center;
      font-size: 11px;
      color: var(--text-muted);
      border-top: 1px solid #f3f4f6;
    }

    .data-subtitle {
      font-size: 15px;
      color: var(--text-muted);
      margin-bottom: 18px;
    }

    .data-links {
      display: flex;
      flex-direction: column;   
      margin-top: 16px;
    }

    .link-bubble {
      display: inline-flex;
      align-items: center;
      padding: 10px 18px;
      border-radius: 10px;
      border: 1px solid var(--gray-border);
      background: #ffffff;
      font-size: 14px;
      color: var(--text-muted);
      cursor: pointer;
      transition: background 0.15s ease, color 0.15s ease, border-color 0.15s ease;
      width: fit-content;
    }

    .link-bubble:hover {
      background: #ecfdf5;
      border-color: var(--green-soft);
      color: var(--green-main);
    }

    .before-after-card {
      margin-top: 16px;
    }

    .before-after-grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 16px;
    }

    .before-after-panel {
      display: flex;
      flex-direction: column;
      gap: 8px;
      font-size: 13px;
      color: var(--text-muted);
    }

    .before-after-panel img {
      width: 100%;
      border-radius: 12px;
      display: block;
      object-fit: cover;
    }

    @media (max-width: 800px) {
      .before-after-grid {
        grid-template-columns: 1fr;
      }
    }
    
    /* Shared image box for intro cards */
      .intro-image {
        width: 100%;
        border-radius: 16px;
        overflow: hidden;           
        border: 1px solid #d1d5db;
        background: #ffffff;
        padding: 4px;             
      }

      /* Image itself */
      .intro-image img {
        display: block;
        width: 100%;
        height: auto;              
        border-radius: 12px;       
      }

      .intro-image.contain img {
        object-fit: contain;       
      }

    .chart-wrapper {
      background: #ffffff;
      border-radius: 18px;
      padding: 16px 18px 12px;
      border: 1px solid #e5e7eb;
      overflow: visible;         
    }

    .chart-wrapper img {
      width: 100%;
      height: auto;                
      display: block;
      border-radius: 12px;   
    }


    .intro-image.tight {
      padding: 6px;               
      border-radius: 14px;       
      border: 1px solid #d1d5db; 
      background: #ffffff;
    }

    .intro-image.tight img {
      border-radius: 12px;       
      object-fit: contain;        
    }


    .gradcam-card {
      margin-top: 24px;
    }

    .gradcam-carousel {
      margin-top: 16px;
      background: #ffffff;
      border-radius: 14px;
      border: 1px solid #e5e7eb;
      padding: 16px 18px 12px;
      max-width: 720px;
      margin-left: auto;
      margin-right: auto;
      position: relative;
    }

    .gradcam-track {
      position: relative;
      overflow: hidden;
    }

    .gradcam-slide {
      display: none;
      text-align: center;
    }

    .gradcam-slide.active {
      display: block;
    }

    .gradcam-slide img {
      max-width: 100%;
      max-height: 320px;      
      border-radius: 10px;
      display: block;
      margin: 0 auto;
    }

    .gradcam-slide figcaption {
      margin-top: 8px;
      font-size: 13px;
      color: #4b5563;
    }

    /* Prev / next buttons */
    .gradcam-nav {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      border: none;
      background: rgba(22, 101, 52, 0.08);
      color: #166534;
      width: 28px;
      height: 28px;
      border-radius: 999px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 18px;
      padding: 0;
    }

    .gradcam-nav:hover {
      background: rgba(22, 101, 52, 0.16);
    }

    .gradcam-nav.prev {
      left: 8px;
    }

    .gradcam-nav.next {
      right: 8px;
    }

    .gradcam-dots {
      margin-top: 10px;
      display: flex;
      justify-content: center;
      gap: 6px;
    }

    .gradcam-dot {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: #d1d5db;
      cursor: pointer;
    }

    .gradcam-dot.active {
      background: #15803d;
    }

     .conclusion-img-box {
    width: 100%;
    max-width: 420px;       
    margin: 20px auto;
    border: 2px solid var(--green-soft);
    border-radius: 14px;
    overflow: hidden;
    padding: 0;
  }

  .conclusion-img-box img {
    width: 100%;
    height: auto;
    display: block;
  }



  </style>
</head>


<body>

 
  <header>
    <div class="logo">♻️ EcoSort</div>
    <nav>
      <a href="#introduction">Introduction</a>
      <a href="#data-prep">Data &amp; Prep</a>
      <a href="#analysis">Analysis</a>
      <a href="#results">Results</a>
      <a href="#conclusion">Conclusion</a>
      <a href="#about">About</a>
      <a href="https://github.com/moukthika-gunapaneedu/EcoSort" target="_blank" rel="noopener noreferrer">
        GitHub Repo
      </a>
    </nav>
  </header>

  <main>

    <!-- Hero -->
    <section class="hero" id="top">
      <div class="section-inner">
        <h1 class="hero-title">EcoSort</h1>
        <p class="hero-tagline">
          An AI-assisted waste classification project that explores how simple images of everyday trash
          can support better recycling, less contamination, and more sustainable cities.
        </p>

        <div class="hero-meta">
          <div class="pill">CNN &amp; Image Classification</div>
          <div class="pill">TrashNet Dataset</div>
          <div class="pill">Recycling &amp; Sustainability</div>
        </div>

        <a href="#introduction" class="scroll-indicator">
          <span>Scroll to begin</span>
          <span class="arrow-down">↓</span>
        </a>
      </div>
    </section>

    <!-- Introduction -->
    <section id="introduction">
    <div class="section-inner">
      <div class="section-label">Section 1 · Introduction</div>
      <h2>Understanding everyday waste decisions</h2>

      <!-- 1. Clearly describe the topic -->
      <div class="card intro-card">
        <div class="intro-text">
          <h3>What is EcoSort about?</h3>
          <p>
            EcoSort focuses on the simple moment when a person stands in front of a bin and tries to decide where an item should go.
            This everyday choice determines whether materials stay in circulation or end up as permanent waste.
            Many people rely on quick visual judgment, which often leads to items being placed in the wrong bin.
            These small errors matter because a single misplaced object can contaminate an entire batch of materials.
            <strong>EcoSort is concerned with these real-world decisions.</strong>
            By looking closely at images of common objects, the project highlights how people interpret waste based on visual cues alone.
            This perspective helps reveal how ordinary choices accumulate into larger patterns that affect communities and the environment.
          </p>
        </div>
        <div class="intro-image contain">
          <img src="website/waste_segregation.png" alt="Waste Segregation">
        </div>
      </div>

      <!-- 2. Topic background -->
      <div class="card intro-card">
        <div class="intro-text">
          <h3>Background: how waste systems work?</h3>
          <p>
            Waste systems begin at a bin but extend through long chains of trucks, workers, and sorting facilities that handle thousands of items each day.
            Once something is thrown away, it enters a process designed to separate what can be reused from what must be discarded.
            These systems rely heavily on items being sorted correctly at the start because early mistakes are difficult to correct later.
            <strong>When contaminated or incorrect items enter recycling streams, entire loads can lose value and be rejected.</strong>
            Rejected loads often end up in landfills even when many items inside were recyclable.
            This outcome increases waste, energy use, and environmental impact.
            Understanding this background shows why small decisions at the bin can have large consequences downstream.
          </p>
        </div>
          <div class="intro-image contain">
            <img src="website/waste_management.png" alt="Waste Management System">
          </div>
      </div>

      <!-- 3. Who is affected -->
      <div class="card intro-card">
        <div class="intro-text">
          <h3>Who is affected by mis-sorted waste?</h3>
          <p>
            Mis-sorted waste affects a wide range of people, even though most never see the consequences directly.
            Residents may believe they recycled correctly, only to learn that contamination caused an entire load to be discarded.
            Workers at sorting facilities face additional effort and safety risks when they must remove items that do not belong.
            Cities and campuses often pay higher fees when loads are rejected or require special handling.
            <strong>These costs ultimately fall on communities that rely on predictable and affordable waste services.</strong>
            Over time, mis-sorting reduces the effectiveness of recycling programs and public trust in the system.
            The combined impact touches households, workers, local budgets, and the environment.
          </p>
        </div>
        <div class="intro-image">
          <img src="website/missorsted.png" alt="Mis-sorted Waste">
        </div>
      </div>

      <!-- 4. Significance and applications -->
      <div class="card intro-card">
        <div class="intro-text">
          <h3>Why does better sorting matter?</h3>
          <p>
            Proper sorting helps keep valuable materials in circulation rather than sending them to landfills.
            When recyclables remain clean and separated, they can be turned into new products with less energy and fewer raw materials.
            This reduces pressure on natural resources such as metals, timber, and fossil fuels.
            <strong>Better sorting also lowers emissions, saves energy, and improves the overall efficiency of recycling systems.</strong>
            Communities and campuses that sort correctly are more likely to meet sustainability goals and reduce waste costs.
            Clean, well-organized waste streams also lead to clearer public spaces and more reliable waste services.
            These benefits show why small choices at the bin carry long-term importance.
          </p>
        </div>
        <div class="intro-image">
          <img src="website/benefits.png" alt="Benefits of Good Sorting">
        </div>
      </div>

      <!-- 5. How images connect to the topic -->
      <div class="card intro-card">
        <div class="intro-text">
          <h3>Where do images fit into this story?</h3>
          <p>
            Images play an important role in understanding how people sort waste because visual appearance shapes most sorting decisions.
            The shape, texture, and condition of an item influence whether a person believes it belongs in recycling, compost, or landfill.
            These details matter because people often rely on quick visual cues rather than reading labels or instructions.
            <strong>By examining many photos of common waste items, EcoSort reveals patterns in how people interpret these objects.</strong>
            Images make it easier to compare what items look like and how easily they might be mistaken for something else.
            This visual perspective provides a simple entry point into understanding everyday sustainability challenges.
            EcoSort uses these images to explore how clearer cues could eventually support better sorting habits.
          </p>
        </div>
        <div class="intro-image">
          <img src="website/mixed.png" alt="Mixed Waste Items">
        </div>
      </div>

    </div>
  </section>


    <!-- Data & Preparation -->
    <section id="data-prep">
      <div class="section-inner">
        <div class="section-label">Section 2</div>
        <h2>Data and preparation</h2>
        <p class="data-subtitle">
          This section describes the dataset behind EcoSort, how the images were gathered and cleaned,
          and how the final collection was organized for analysis.
        </p>

        <!-- 1. Dataset overview -->
        <div class="card">
          <div class="two-column">
            <div>
              <h3>What dataset does EcoSort use?</h3>
              <p style="text-align: justify;">
                EcoSort is built on a public image collection known as the
                <strong>TrashNet dataset</strong>. TrashNet contains photographs of everyday
                waste items placed on simple backgrounds and grouped into <strong>six categories:
                cardboard, glass, metal, paper, plastic, and mixed trash.</strong> Each image shows
                a single object, such as a bottle, can, or piece of cardboard, which makes
                it easier to see the visual cues people might rely on when sorting.
                Because the backgrounds are clean and consistent, the images invite the
                reader to focus on the material itself rather than clutter in the scene.
                This makes TrashNet a practical starting point for exploring how common
                materials actually look at the moment someone decides where they belong.
              </p>

              <div class="data-links" style="margin-top: 14px;">
                <a class="link-bubble"
                  href="https://github.com/garythung/trashnet"
                  target="_blank" rel="noopener noreferrer"
                  style="font-weight: 600; border-color: var(--green-soft);">
                  View original TrashNet dataset
                </a>
              </div>

            <div class="data-links" style="margin-top: 14px;">
              <a class="link-bubble"
                href="https://www.kaggle.com/datasets/feyzazkefe/trashnet"
                target="_blank" rel="noopener noreferrer"
                style="font-weight: 600; border-color: var(--green-soft);">
                Download resized TrashNet version (Kaggle)
              </a>
            </div>

            </div>

            <div>
              <div class="intro-image">
                <img src="website/trashnet.png" alt="Example images from each TrashNet class">
              </div>
            </div>
          </div>
        </div>

        <!-- 2. Raw structure and first checks -->
          <div class="card" style="margin-top: 20px;">
            <h3>What does the raw dataset look like?</h3>
            <p style="text-align: justify;">
              The raw TrashNet dataset is organized into six folders, one for each material type. 
              Before any cleaning or splitting, a quick look at the dataset helps understand what the model will see.
            </p>

            <ul style="margin-top: 14px; line-height: 1.6;">
              <li><strong>Total images:</strong> 2,527 files across all classes.</li>
              <li><strong>Number of classes:</strong> 6 (cardboard, glass, metal, paper, plastic, trash).</li>
              <li><strong>Raw folder structure:</strong> one folder per class, each containing various lighting, shape, and condition variations.</li>
              <li><strong>Typical image resolution:</strong> varies widely (e.g., 384×512, 512×384, etc.) before resizing.</li>
              <li><strong>File formats:</strong> mix of JPG and PNG images.</li>
            </ul>

            <div class="data-links" style="margin-top: 14px;">
              <a class="link-bubble"
                href="https://github.com/moukthika-gunapaneedu/EcoSort/tree/main/data/sample_raw"
                target="_blank" rel="noopener noreferrer"
                style="font-weight: 600; border-color: var(--green-soft);">
                View sample of the raw image folders (EcoSort)
              </a>
            </div>


            <p style="margin-top: 10px; font-size: 13px; color: #6b7280;">
              Note: The full raw TrashNet dataset used for training is stored locally and excluded from the repository
              because of its size and Git ignore settings. The linked folder shows a small sample of the original
              folder structure; all analysis in EcoSort was run on the complete dataset.
            </p>
          </div>

          <div class="card before-after-card">
          <h3>How are the classes distributed?</h3>

          <div class="chart-wrapper">
            <img src="website/trash_class_counts.png"
                alt="Bar chart showing the number of images in each TrashNet category">
          </div>

          <p style="margin-top: 14px; text-align: justify;">
            The bar chart shows how many images are available for each category in the dataset.
            Some classes, such as <strong>paper</strong> and <strong>plastic</strong>, have noticeably
            more samples, while others like <strong>trash</strong> and <strong>metal</strong> are smaller
            and more varied. This imbalance matters because it shapes what the model sees most often
            during training: large classes provide strong, consistent patterns, while smaller classes
            demand more careful augmentation and evaluation to avoid the model defaulting to the majority
            categories.
          </p>
          <p>
            Keeping this distribution in mind is important when interpreting EcoSort’s performance.
            High overall accuracy can hide weaker performance on underrepresented classes, so later
            sections always report results per class, not just a single aggregate score.
          </p>
        </div>

      
        <div class="card" style="margin-top: 20px;">
            <div>
              <h3>How were the images cleaned and prepared?</h3>
              <p style="text-align: justify;">
                Before EcoSort can draw any conclusions, the images need to be cleaned and
                brought into a consistent format.
                <ul style="margin-top: 12px;">
                  <li>All images are loaded from their class folders and checked to ensure they contain valid pixel data.</li>
                  <li>Files are resized to a consistent resolution so each sample has the same dimensions during training.</li>
                  <li>Images are converted to RGB format to avoid inconsistencies from grayscale or multi-channel inputs.</li>
                  <li>Orientation and clarity are verified so that the main object remains visible and centered.</li>
                  <li>Obvious duplicates or corrupted images are filtered out to prevent noisy samples from entering the dataset.</li>
                  <li>Labels are standardized using folder names so each image can be traced cleanly back to its category.</li>
                  <li>The cleaned images are then saved into structured folders (train/val/test) for consistent processing throughout the pipeline.</li>
                </ul>
              </p>

              <div class="data-links" style="margin-top: 14px;">
                <a class="link-bubble"
                  href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/data_prep.py"
                  target="_blank" rel="noopener noreferrer"
                  style="font-weight: 600; border-color: var(--green-soft);">
                  View the cleaning and preprocessing code
                </a>
              </div>

            </div>
        </div>

        
        <div class="card before-after-card">
          <h3>Example: before and after cleaning</h3>
          <div class="before-after-grid">
            <div class="before-after-panel">
              <span><strong>Before cleaning</strong></span>
              <img src="website/Before.png" alt="Example raw TrashNet image before cleaning">
            </div>
            <div class="before-after-panel">
              <span><strong>After cleaning</strong></span>
              <img src="website/After.png" alt="Same image after resizing and cleaning">
            </div>
          </div>
          <div class="image-info-row" style="display: flex; justify-content: space-between; margin-top: 12px;">

          
          <div style="width: 48%; text-align: center;">
            <p style="margin: 0; font-weight: 600;">Original resolution</p>
            <p style="margin: 4px 0 0 0; color: #555;">512 × 384 pixels (varies by sample)</p>
          </div>

          
          <div style="width: 48%; text-align: center;">
            <p style="margin: 0; font-weight: 600;">Processed resolution</p>
            <p style="margin: 4px 0 0 0; color: #555;">224 × 224 pixels (uniform)</p>
          </div>

        </div>

          <p style="margin-top: 10px;">
            These side-by-side examples illustrate how the same item looks before and after basic
            preparation.
          </p>
          <p style="text-align: justify;">
            The processed images look very similar to the raw images because the preprocessing step is designed 
            to standardize the data rather than alter it. EcoSort does not apply any heavy transformations at 
            this stage. Instead, images are cleaned, resized, and converted to a consistent RGB format while 
            preserving their original appearance. This ensures that each sample remains visually true to the 
            real object while still being uniform enough for the model to learn from effectively.
          </p>
        </div>

        <div class="card" style="margin-top: 20px;">
          <h3>How was the dataset organized for exploration?</h3>

          <p style="text-align: justify;">
            After cleaning, the dataset is divided into three groups that each serve a specific purpose in the
            exploration and evaluation process. This structure ensures that the model learns from one portion of
            the data, is checked on another, and is finally measured on completely unseen examples.
          </p>

          <ul style="margin-top: 14px; line-height: 1.6;">
            <li>
              <strong>Training set (80%):</strong> the largest split, used to teach the model what typical examples of 
              each category look like.
            </li>
            <li>
              <strong>Validation set (10%):</strong> a checkpoint split that verifies whether the learned 
              patterns still hold on new images the model did not train on.
            </li>
            <li>
              <strong>Test set (10%):</strong> kept completely separate until final evaluation to measure true 
              generalization on unseen samples.
            </li>
          </ul>

          <p style="text-align: justify; margin-top: 10px;">
            Splitting the data this way mirrors a natural learning process: study a subset, periodically check your
            understanding, and then test that knowledge in a new setting. This approach reduces over-fitting, keeps 
            the evaluation fair, and ensures the model is judged on its ability to generalize rather than memorize.
          </p>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/split_dataset.py"
              target="_blank" rel="noopener noreferrer"
              style="font-weight: 600; border-color: var(--green-soft);">
              View the dataset splitting code
            </a>
          </div>

        </div>


        <div class="card" style="margin-top: 20px;">
          <h3>How does the data look after cleaning?</h3>

          <p style="text-align: justify;">
            After preparing the dataset, each category was rechecked for image quality, duplicates, and valid 
            file types. The cleaned dataset is slightly smaller than the raw version because blurred, unusable, 
            or mislabeled images were removed. The bar chart below shows the new distribution across all six categories.
          </p>

          <div class="chart-wrapper" style="margin-top: 20px;">
            <img src="website/after_class_counts.png" alt="After-cleaning class distribution bar chart">
          </div>

          <p style="margin-top: 14px; text-align: justify;">
            The processed dataset preserves the same six-category structure as the original, but all images are now 
            standardized and cleaned. Each file has been resized, converted to RGB, checked for readability, and 
            verified to ensure the main object is clearly visible. This results in a more uniform and reliable set of 
            samples for model training.
          </p>

          <p style="text-align: justify;">
            While the overall balance across categories remains similar, certain classes became slightly smaller after 
            removing corrupted or low-quality files. These adjustments help reduce noise in the training data and 
            improve the model’s ability to learn consistent visual patterns.
          </p>

          <!-- Processed dataset bullet list -->
          <ul style="margin-top: 14px; line-height: 1.6;">
            <li><strong>Total processed images:</strong> 2019 files after removing duplicates, corrupted files, and unusable samples.</li>
            <li><strong>Number of classes:</strong> 6 (cardboard, glass, metal, paper, plastic, trash).</li>
            <li><strong>Processed folder structure:</strong> organized into <strong>train</strong>, <strong>val</strong>, and <strong>test</strong> splits, each containing all six categories.</li>
            <li><strong>Final image resolution:</strong> all images resized to a uniform <strong>224 × 224</strong> RGB format.</li>
            <li><strong>File formats:</strong> cleaned and saved as consistent <strong>RGB JPG</strong> images.</li>
          </ul>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/tree/main/data/sample_processed"
              target="_blank" rel="noopener noreferrer"
              style="font-weight: 600; border-color: var(--green-soft);">
              View sample of the cleaned (processed) dataset folders
            </a>
          </div>


          <p style="margin-top: 10px; font-size: 13px; color: #6b7280;">
            Note: Only a small sample of the processed dataset is shown here.  
            The full processed dataset used for training is stored locally and excluded from the repository 
            due to its size and Git ignore settings.
          </p>
        </div>

        <div class="card" style="margin-top: 20px;">
          <h3>Data and code access</h3>

          <p style="text-align: justify;">
            To keep EcoSort reproducible and transparent, the entire project (including the sample raw data,
            sample processed data, preprocessing scripts, splitting code, training pipeline, and visualizations)
            is available in the GitHub repository. Only small samples of the dataset are included in the repo.
            The full raw and processed datasets used for training are stored locally due to their size and
            Git ignore settings.
          </p>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort"
              target="_blank" rel="noopener noreferrer"
              style="font-weight: 600; border-color: var(--green-soft);">
              EcoSort GitHub Repository (project code & sample data)
            </a>
          </div>
        </div>

      </div>
    </section>


    <section id="analysis">
      <div class="section-inner">
        <div class="section-label">Section 3 · Analysis · Models &amp; Methods</div>

        <!-- 3.1 Why this type of model? -->
         <h2>Teaching a model to “see” materials</h2>
        <div class="card" style="margin-top: 20px;">

          <p style="text-align: justify;">
            EcoSort uses <strong>convolutional neural networks (CNNs)</strong> to recognize materials from
            images. CNNs are a natural choice for this task because they are designed to look for local
            patterns in pixels (edges, textures, and shapes) that are strong indicators of whether an object is
            made of cardboard, glass, metal, paper, plastic, or mixed trash. Rather than manually designing
            features, the network learns filters that highlight what matters most in each class.
          </p>

          <p style="text-align: justify;">
            Two related architectures were explored. A <strong>custom EcoSortCNN</strong> was used as a
            baseline, built from stacked convolution, batch normalization, and pooling layers followed by a
            small fully connected classifier. The final model is a <strong>ResNet-18 based network</strong>,
            which starts from a backbone pretrained on ImageNet and then fine-tunes it for EcoSort’s six
            categories. Using a pretrained backbone allows the model to reuse rich, general visual features
            (such as corners, textures, and object parts) and adapt them to the more specific task of
            distinguishing common waste items.
          </p>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/model.py"
              target="_blank" rel="noopener noreferrer"
              style="font-weight: 600; border-color: var(--green-soft);">
              View model code (model.py)
            </a>
          </div>
        </div>

        <div class="card" style="margin-top: 20px;">
          <h3>What is a Convolutional Neural Network (CNN)?</h3>

          <p style="text-align: justify;">
            A <strong>Convolutional Neural Network (CNN)</strong> is a type of deep learning model that is
            especially good at understanding images. Instead of looking at the entire image at once,
            a CNN scans it using small filters that move across pixels. These filters learn to detect
            important visual patterns such as edges, corners, curves, and textures. As the image flows
            through deeper layers, the model shifts from detecting simple shapes to recognizing
            more complex structures like bottle rims, shiny reflections, cardboard surfaces, and so on.
          </p>

          <ul style="line-height: 1.6;">
            <li><strong>Convolution layers:</strong> learn small patterns in small regions of the image.</li>
            <li><strong>ReLU activation:</strong> keeps only the useful parts of the signal.</li>
            <li><strong>Pooling layers:</strong> shrink the image while keeping the important regions.</li>
            <li><strong>Fully connected layer:</strong> uses all extracted features to make a final prediction.</li>
          </ul>

          <div class="intro-image tight" style="margin-top: 14px;">
            <!-- Replace with your own image -->
            <img src="website/cnn.png"
                alt="General CNN diagram showing convolution, activation, pooling, flattening, and fully-connected layers.">
          </div>

          <p style="margin-top: 6px; font-size: 13px; color: #6b7280;">
            Illustration: A typical CNN pipeline for image classification.
          </p>
        </div>

        <div class="card" style="margin-top: 20px;">
          <h3>What is the Custom EcoSortCNN?</h3>

          <p style="text-align: justify;">
            The <strong>Custom EcoSortCNN</strong> is a lightweight convolutional neural network built
            specifically for this project. It serves as a <strong>baseline model</strong>, showing how well
            a straightforward CNN can perform before introducing more advanced techniques like transfer
            learning. Its design follows the classic CNN structure: early layers extract patterns from the
            image, and the final layers turn those patterns into a prediction for one of the six material
            categories.
          </p>

          <!-- Text diagram of the architecture -->
          <pre style="
              margin-top: 14px;
              padding: 14px 16px;
              background: #ffffff;
              border-radius: 10px;
              border: 2px solid #15803d33;     /* subtle green (20% opacity) */
              box-shadow: 0 0 3px rgba(21, 128, 61, 0.08); /* soft shadow */
              font-family: 'JetBrains Mono', 'SFMono-Regular', Menlo, Monaco, Consolas, monospace;
              font-size: 12.5px;
              overflow-x: auto;
              white-space: pre;
              color: #374151;
          ">
        Input Image (224 × 224 × 3)
                    ↓
        ──────────────────────────────────────────────
        Block 1:  Conv(32 filters, 3×3)
                  BatchNorm
                  ReLU
                  MaxPool(2×2)
                    ↓
        Block 2:  Conv(64 filters, 3×3)
                  BatchNorm
                  ReLU
                  MaxPool(2×2)
                    ↓
        Block 3:  Conv(128 filters, 3×3)
                  BatchNorm
                  ReLU
                  MaxPool(2×2)
                    ↓
        Block 4:  Conv(256 filters, 3×3)
                  BatchNorm
                  ReLU
                  MaxPool(2×2)
                    ↓
        ──────────────────────────────────────────────
        Flatten
                    ↓
        Fully Connected Layer (512 units)
                    ↓
        Dropout (0.5)
                    ↓
        Output Layer (6 material classes)
          </pre>

          <ul style="margin-top: 14px; line-height: 1.6; color: #4b5563; font-size: 15px;">
            <li><strong>4 convolution blocks:</strong> each block has Conv + BatchNorm + ReLU + MaxPool.</li>
            <li><strong>Channels grow:</strong> 32 → 64 → 128 → 256, allowing the model to detect increasingly complex patterns.</li>
            <li><strong>Dense classifier:</strong> flatten → 512-unit fully connected layer → dropout → 6-class output layer.</li>
            <li><strong>Trained from scratch:</strong> all weights are learned only from EcoSort’s dataset.</li>
            <li><strong>Role:</strong> provides a clean baseline to compare against more powerful models such as ResNet-18.</li>
          </ul>

          <h4 style="margin-top: 18px; font-size: 15px;">What do the building blocks mean?</h4>

          <ul style="margin-top: 8px; line-height: 1.6; color: #4b5563; font-size: 15px;">
            <li>
              <strong>Convolution (Conv):</strong> slides small filters over the image to detect local
              patterns such as edges, corners, and textures.
            </li>
            <li>
              <strong>Batch Normalization (BatchNorm):</strong> keeps the activations in a stable range so
              the network trains faster and is less likely to overfit.
            </li>
            <li>
              <strong>ReLU activation:</strong> keeps only positive values and sets negative values to zero,
              helping the model learn non-linear patterns.
            </li>
            <li>
              <strong>Max Pooling (MaxPool):</strong> reduces the spatial size by keeping only the strongest
              responses in each small region, which focuses the model on the most important visual cues.
            </li>
          </ul>

          <p style="margin-top: 10px; text-align: justify; color: #4b5563; font-size: 15px;">
            The Custom EcoSortCNN essentially starts by noticing basic shapes and textures in each
            image and gradually builds up to more detailed features. The final layers combine these features
            to decide whether the object looks most like cardboard, glass, metal, paper, plastic, or mixed
            trash. This baseline model shows how far a standard CNN can go before adding more advanced
            architectures like ResNet-18.
          </p>
        </div>



        <div class="card" style="margin-top: 20px;">

          <h3>What is ResNet-18?</h3>

          <p style="text-align: justify;">
            <strong>ResNet-18</strong> is a widely used deep convolutional neural network that introduced
            the concept of <strong>residual connections</strong>. These skip-connections act like shortcut
            paths, allowing information to flow forward without being lost. This makes the network much
            easier to train, especially as it becomes deeper. ResNet-18 is powerful, stable, and still
            lightweight enough for fast training and real-time applications.
          </p>

          <p style="text-align: justify;">
            In EcoSort, ResNet-18 is used through <strong>transfer learning</strong>. The model starts with
            weights pretrained on ImageNet, a huge dataset of everyday images. These pretrained weights
            already understand edges, textures, shapes, and object structures. We replace the final layer so
            the model predicts EcoSort’s six material categories, then fine-tune the entire network on the
            cleaned dataset.
          </p>

         
          <pre style="
              margin-top: 14px;
              padding: 14px 16px;
              background: #ffffff;
              border-radius: 10px;
              border: 2px solid #15803d33;        /* subtle green outline */
              box-shadow: 0 0 3px rgba(21, 128, 61, 0.08); /* soft green shadow */
              font-family: 'JetBrains Mono', 'SFMono-Regular', Menlo, Monaco, Consolas, monospace;
              font-size: 12.5px;
              overflow-x: auto;
              white-space: pre;
              color: #374151;
          ">
        Input Image (224 × 224 × 3)
                        ↓
        Conv Layer + BatchNorm + ReLU
                        ↓
        ──────────── Residual Block (x2) ────────────
            Conv → BN → ReLU → Conv → BN
            + skip connection (input added back)
                        ↓
        ──────────── Residual Block (x2) ────────────
            Conv → BN → ReLU → Conv → BN
            + skip connection
                        ↓
        ──────────── Residual Block (x2) ────────────
                        ↓
        ──────────── Residual Block (x2) ────────────
                        ↓
        Global Average Pooling
                        ↓
        Fully Connected Layer (6 classes)
          </pre>

          <ul style="margin-top: 14px; line-height: 1.6; color: #4b5563; font-size: 15px;">
            <li><strong>Residual blocks:</strong> solve training problems in deep networks by using skip connections.</li>
            <li><strong>Pretrained backbone:</strong> begins with ImageNet features learned from millions of images.</li>
            <li><strong>EcoSort fine-tuning:</strong> replaces the final classification layer with a 6-class head.</li>
            <li><strong>Advantages:</strong> faster training, higher accuracy, better generalization, more stable gradients.</li>
          </ul>

          <h4 style="margin-top: 18px; font-size: 15px;">What is a residual connection?</h4>

          <p style="text-align: justify; color: #4b5563; font-size: 15px;">
            A residual connection allows a block to output:
            <strong>BlockOutput = F(input) + input</strong>.
            This simple addition prevents the network from “forgetting” information and makes deep networks
            learn far more effectively. Instead of struggling to learn everything from scratch, the block
            only learns the “difference” from the input.
          </p>

          <p style="margin-top: 10px; text-align: justify; color: #4b5563; font-size: 15px;">
            ResNet-18 is like giving the model a shortcut so it can keep what it already
            knows while learning new details. This makes it extremely reliable for EcoSort, where the model
            must distinguish fine textures such as plastic gloss, metal shine, paper fibers, and cardboard
            grain.
          </p>
        </div>

        <div class="card" style="margin-top: 24px; padding: 28px 32px;">
          <h3 style="margin-bottom: 16px;">Why were these models used?</h3>

          <p style="text-align: justify; line-height: 1.65; margin-bottom: 18px;">
            EcoSort uses two complementary models - a custom CNN and a ResNet-18 backbone because each contributes
            something different to the overall system. Together, they allow the project to compare a simple,
            from-scratch architecture with a stronger transfer-learning model that is better suited for
            real-world accuracy.
          </p>

          <ul style="line-height: 1.7; font-size: 15px; color: #4b5563; padding-left: 20px; text-align: justify;">
            <li style="margin-bottom: 14px;">
              <strong>Custom EcoSortCNN (baseline model):</strong>
              A lightweight, easy-to-train network designed specifically for the EcoSort dataset.
              It serves as a controlled reference point that shows how well a standard CNN performs without
              external knowledge from large datasets. This makes it ideal for understanding the dataset’s
              difficulty before introducing more advanced techniques.
            </li>

            <li style="margin-bottom: 14px;">
              <strong>ResNet-18 (transfer-learning model):</strong>
              A proven architecture pretrained on ImageNet, which already captures patterns such as edges,
              shapes, textures, and object structures. Fine-tuning this model allows EcoSort to benefit from
              strong visual features learned from millions of images, resulting in faster convergence and
              higher accuracy on small or class-imbalanced datasets.
            </li>

            <li style="margin-bottom: 14px;">
              <strong>Balanced comparison:</strong>
              Using both models allows EcoSort to measure the impact of transfer learning,
              identify when a simple CNN struggles, and see how much performance gain comes from deeper
              architectures with residual connections.
            </li>

            <li style="margin-bottom: 4px;">
              <strong>Practical considerations:</strong>
              ResNet-18 offers an excellent trade-off between speed and accuracy, making it suitable for a
              student project environment while still achieving production-quality insights. The custom CNN
              remains useful for experimentation, debugging, and lightweight deployments.
            </li>
          </ul>

          <p style="text-align: justify; line-height: 1.65; margin-top: 18px; color: #4b5563;">
            The custom CNN provides a clean, interpretable baseline, while ResNet-18 delivers the
            accuracy and generalization needed for reliable real-world waste classification. Comparing the two
            helps highlight the strengths and limitations of each approach.
          </p>
        </div>





        <!-- 3.2 Model architectures -->
        <div class="card" style="margin-top: 24px;">
          <h3>What do the EcoSort models look like structurally?</h3>

          <p style="text-align: justify;">
            The <strong>EcoSortCNN baseline</strong> processes each 224 × 224 RGB image through a sequence of
            four convolutional blocks. Each block applies 3×3 filters, batch normalization, a ReLU activation,
            and a 2×2 max-pooling layer, gradually increasing the number of channels
            (32 → 64 → 128 → 256) while reducing the spatial resolution. The resulting feature maps are then
            flattened and passed through a 512-unit dense layer with dropout before the final six-class output
            layer. This model is deliberately compact: it is easy to train from scratch and provides a clear
            reference point for how much value transfer learning adds.
          </p>

          <p style="text-align: justify;">
            The <strong>EcoSortResNet18 model</strong> replaces the hand-built feature extractor with a
            <em>ResNet-18</em> backbone that has already been trained on ImageNet. The original final layer is
            removed and replaced with a new fully connected head that outputs six logits, one for each
            EcoSort category. In this project, the backbone is <strong>fine-tuned end-to-end</strong> rather than
            frozen, allowing the earlier layers to adjust slightly to the specific textures and shapes present
            in waste images while still benefiting from the strong initialization provided by pretraining.
          </p>

          <ul style="margin-top: 10px; line-height: 1.6;">
            <li><strong>Input:</strong> 224 × 224 RGB images, normalized with ImageNet mean and standard deviation.</li>
            <li><strong>Backbone:</strong> ResNet-18 with residual blocks that help train deeper networks reliably.</li>
            <li><strong>Output head:</strong> final fully connected layer mapping to six material classes.</li>
            <li><strong>Choice of model:</strong> ResNet-18 offers a good balance of accuracy and computational
                cost for a project-scale dataset like TrashNet.</li>
          </ul>
        </div>

        

        <!-- 3.3 Training setup and optimization -->
        <div class="card" style="margin-top: 24px;">
          <h3>How is the model trained?</h3>

          <p style="text-align: justify;">
            EcoSort is trained on the cleaned and split dataset using the same train/validation/test structure
            described earlier. Batches of images and labels are loaded from disk, moved to the GPU when
            available, and passed through the network to produce class scores. The training loop then measures
            how far the predictions are from the true labels and uses this feedback to update the model.
          </p>

          <ul style="margin-top: 10px; line-height: 1.6;">
            <li>
              <strong>Loss function:</strong> cross-entropy with a small amount of label smoothing
              (<code>0.05</code>), which slightly softens the target distribution. This helps prevent the model
              from becoming overly confident and improves generalization on the smaller “trash” class.
            </li>
            <li>
              <strong>Optimizer:</strong> AdamW with a learning rate of <code>3 × 10⁻⁴</code> and weight decay
              <code>1 × 10⁻⁴</code>, applied only to parameters that are marked as trainable. Weight decay acts as
              a gentle regularizer to discourage overly large weights.
            </li>
            <li>
              <strong>Learning-rate scheduler:</strong> a ReduceLROnPlateau scheduler monitors validation
              accuracy and reduces the learning rate when progress stalls, allowing larger, exploratory steps
              early in training and finer adjustments later on.
            </li>
            <li>
              <strong>Training length:</strong> up to 20 epochs, with the best model snapshot saved whenever the
              validation accuracy improves. This ensures that later evaluations always use the strongest version
              of the network rather than just the last epoch.
            </li>
          </ul>

          <p style="text-align: justify; margin-top: 10px;">
            During both training and validation, EcoSort tracks average loss and accuracy across each epoch.
            These metrics are used to compare architectures, tune hyperparameters, and check for signs of
            over-fitting, such as validation accuracy plateauing while training accuracy continues to rise.
          </p>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/train.py"
              target="_blank" rel="noopener noreferrer"
              style="font-weight: 600; border-color: var(--green-soft);">
              View the training loop (train.py)
            </a>
          </div>


          <!-- Training Log (as text image style) -->
<pre style="
    margin-top: 18px;
    padding: 14px 16px;
    background: #ffffff;
    border-radius: 10px;
    border: 2px solid #15803d33;        /* subtle green outline */
    box-shadow: 0 0 3px rgba(21, 128, 61, 0.08); /* soft green shadow */
    font-family: 'JetBrains Mono','SFMono-Regular',Menlo,Monaco,Consolas,monospace;
    font-size: 12.5px;
    overflow-x: auto;
    white-space: pre;
    color: #374151;
">

Epoch 1/20  - 348.0s | Train Loss: 0.9469 | Train Acc: 0.7162 | Val Loss: 0.7927 | Val Acc: 0.8008
Epoch 2/20  - 340.1s | Train Loss: 0.6748 | Train Acc: 0.8262 | Val Loss: 0.8419 | Val Acc: 0.7729
Epoch 3/20  - 336.3s | Train Loss: 0.5755 | Train Acc: 0.8653 | Val Loss: 0.6882 | Val Acc: 0.8446
Epoch 4/20  - 337.9s | Train Loss: 0.5204 | Train Acc: 0.8975 | Val Loss: 0.6327 | Val Acc: 0.8367
Epoch 5/20  - 336.3s | Train Loss: 0.4359 | Train Acc: 0.9356 | Val Loss: 0.7124 | Val Acc: 0.8327
Epoch 6/20  - 338.9s | Train Loss: 0.4575 | Train Acc: 0.9312 | Val Loss: 0.5779 | Val Acc: 0.8845
Epoch 7/20  - 334.5s | Train Loss: 0.4033 | Train Acc: 0.9475 | Val Loss: 0.6926 | Val Acc: 0.8566
Epoch 8/20  - 335.2s | Train Loss: 0.4112 | Train Acc: 0.9455 | Val Loss: 0.5951 | Val Acc: 0.8845
Epoch 9/20  - 334.3s | Train Loss: 0.3664 | Train Acc: 0.9658 | Val Loss: 0.5065 | Val Acc: 0.8924
Epoch 10/20 - 335.8s | Train Loss: 0.3538 | Train Acc: 0.9728 | Val Loss: 0.5375 | Val Acc: 0.8924
Epoch 11/20 - 340.3s | Train Loss: 0.3316 | Train Acc: 0.9767 | Val Loss: 0.5097 | Val Acc: 0.9163
Epoch 12/20 - 342.9s | Train Loss: 0.3133 | Train Acc: 0.9861 | Val Loss: 0.5058 | Val Acc: 0.9243
Epoch 13/20 - 339.7s | Train Loss: 0.3356 | Train Acc: 0.9762 | Val Loss: 0.5007 | Val Acc: 0.9124
Epoch 14/20 - 336.1s | Train Loss: 0.3692 | Train Acc: 0.9614 | Val Loss: 0.6553 | Val Acc: 0.8367
Epoch 15/20 - 337.2s | Train Loss: 0.3634 | Train Acc: 0.9658 | Val Loss: 0.5877 | Val Acc: 0.9044
Epoch 16/20 - 337.1s | Train Loss: 0.3156 | Train Acc: 0.9886 | Val Loss: 0.4615 | Val Acc: 0.9323
Epoch 17/20 - 335.9s | Train Loss: 0.2886 | Train Acc: 0.9955 | Val Loss: 0.4603 | Val Acc: 0.9203
Epoch 18/20 - 338.4s | Train Loss: 0.2879 | Train Acc: 0.9921 | Val Loss: 0.4578 | Val Acc: 0.9283
Epoch 19/20 - 336.8s | Train Loss: 0.2803 | Train Acc: 0.9970 | Val Loss: 0.4297 | Val Acc: 0.9363
Epoch 20/20 - 335.2s | Train Loss: 0.2746 | Train Acc: 0.9965 | Val Loss: 0.4153 | Val Acc: 0.9482

Best validation accuracy: 0.9482
Best model saved to: results/model_weights/ecosort_cnn_best.pth
</pre>

        </div>


        <div class="card" style="margin-top: 22px;">
          <h3 style="margin-bottom: 12px;">How is the trained model used?</h3>

          <p style="text-align: justify;">
            After training, EcoSort loads the best saved model checkpoint and uses it to classify 
            <strong>any new waste image</strong>. This stage is called inference. The goal is simple:
            take one image, process it, and return the most likely material class.
          </p>

          <ol style="margin-top: 12px; padding-left: 22px; text-align: justify;">
            <li><strong>Load the model checkpoint:</strong> EcoSort restores the trained weights so the 
                model behaves exactly as it did after training.</li>

            <li><strong>Preprocess the image:</strong> Resize to 224×224, convert to RGB, normalize with 
                ImageNet statistics, and transform into a tensor.</li>

            <li><strong>Switch to inference mode:</strong> The model is set to 
                <code>eval()</code> so BatchNorm and Dropout behave deterministically.</li>

            <li><strong>Run a forward pass:</strong> The image tensor is passed through the network to 
                produce six logits - one for each EcoSort category.</li>

            <li><strong>Convert logits to probabilities:</strong> A softmax layer transforms these scores 
                into probabilities that sum to 1.</li>

            <li><strong>Select the highest probability:</strong> The class with the largest probability 
                becomes the final predicted label (e.g., “paper,” “plastic").</li>
          </ol>

          <p style="text-align: justify; margin-top: 12px;">
            This inference pipeline is how EcoSort would operate in real-world applications - classifying 
            images one at a time in recycling systems, sustainability apps, or automated waste-sorting tools.
          </p>

          <!-- Diagram block -->
          <div style="
              margin-top: 20px;
              padding: 14px 16px;
              border: 1.5px solid var(--green-soft);
              border-radius: 10px;
              font-family: monospace;
              background: #ffffff;">
        <pre style="margin: 0; white-space: pre-wrap; font-size: 14px;">
        Image 
          → Preprocessing (resize, RGB, normalize)
            → Model Forward Pass
              → Softmax Probabilities
                → Predicted Material Label
        </pre>
          </div>

        </div>
    </section>

    <section id="results">
      <div class="section-inner">
        <div class="section-label">Section 4 · Results</div>
        <h2>How well does EcoSort perform?</h2>

        <div class="card">
          <h3>Performance on the held-out test set</h3>

          <p style="text-align: justify;">
            After training, EcoSort is evaluated on a <strong>held-out test split</strong> that the model never sees during
            training or validation. The best checkpoint from the training loop is reloaded, and the model is run once over
            all test images to obtain final predictions. For each image, EcoSort compares the predicted material label with
            the true label, building up a complete picture of how often the model is correct and where mistakes occur.
          </p>

          <p style="text-align: justify;">
            The figure below shows the resulting <strong>confusion matrix</strong>, which counts how many times each true
            class (rows) is predicted as each possible class (columns). Darker green squares along the diagonal indicate
            correct predictions, while off-diagonal cells show misclassifications. This view complements overall accuracy
            by revealing which materials are consistently recognized and which tend to be confused with others.
          </p>

          <!-- Confusion matrix image -->
           <div style="width: 70%; margin: 20px auto; border: 1px solid var(--green-soft); border-radius: 12px; overflow: hidden;">
              <img src="results/confusion_matrix/confusion_matrix.png"
                  alt="EcoSort confusion matrix showing true vs predicted labels"
                  style="width: 100%; display: block;">
            </div>

          <p style="margin-top: 14px; text-align: justify; color: #4b5563; font-size: 15px;">
            In EcoSort’s confusion matrix, most of the mass lies on the diagonal, indicating that the model correctly
            distinguishes the six material types in the majority of test cases. Classes such as <strong>paper</strong>,
            <strong>cardboard</strong>, and <strong>plastic</strong> show very strong diagonals with almost no confusion.
            The few off-diagonal entries mainly occur between visually similar categories. For example,
            <strong>glass</strong> and <strong>metal</strong>, or occasional confusion between <strong>plastic</strong>
            and <strong>trash</strong>, where shape and surface cues can overlap. These patterns mirror real-world sorting
            challenges, where shiny or crumpled items are more easily misinterpreted.
          </p>

          <p style="margin-top: 16px; text-align: justify;">
            Alongside the confusion matrix, a <strong>classification report</strong> summarizes precision, recall, and
            F1-score for each class. These metrics show how the model performs on both common categories and smaller,
            more challenging ones such as “trash,” where data is more limited and visual appearance is more varied.
          </p>

          
          <pre style="
              margin-top: 14px;
              padding: 14px 16px;
              background: #ffffff;
              border-radius: 10px;
              border: 2px solid #15803d33;
              box-shadow: 0 0 3px rgba(21, 128, 61, 0.08);
              font-family: 'JetBrains Mono','SFMono-Regular',Menlo,Monaco,Consolas,monospace;
              font-size: 12.5px;
              overflow-x: auto;
              white-space: pre;
              color: #374151;
          ">
        precision    recall  f1-score   support

        cardboard     0.9756    0.9756    0.9756        41
        glass         0.9423    0.9608    0.9515        51
        metal         0.9286    0.9512    0.9398        41
        paper         0.9833    0.9833    0.9833        60
        plastic       0.9787    0.9388    0.9583        49
        trash         0.9333    0.9333    0.9333        15

        accuracy                          0.9611       257
        macro avg     0.9570    0.9572    0.9570       257
        weighted avg  0.9614    0.9611    0.9611       257
          </pre>

          <p style="margin-top: 10px; text-align: justify; color: #4b5563; font-size: 15px;">
            The report confirms that EcoSort achieves <strong>around 96% overall accuracy</strong>, with high precision and
            recall across all six materials. Even the smallest class, <strong>trash</strong>, maintains strong scores, which
            suggests that the model is not simply favoring the more common categories. The close agreement between precision,
            recall, and F1 across classes indicates balanced behavior: EcoSort rarely over-predicts a class and is similarly
            unlikely to miss it when it appears.
          </p>

          <!-- Link bubbles -->

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/evaluate.py"
              target="_blank" rel="noopener noreferrer",
              style="font-weight: 600; border-color: var(--green-soft);">
              View evaluation script (evaluate.py)
            </a> <br>

            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/results/classification_report.txt"
              target="_blank" rel="noopener noreferrer",
              style="font-weight: 600; border-color: var(--green-soft);">
              View full classification report (precision · recall · F1)
            </a>
          </div>
        </div>


        <!-- 4.2 Class-wise behaviour -->
        <div class="card" style="margin-top: 24px;">
          <h3>Which materials are easiest and hardest?</h3>

          <p style="text-align: justify;">
            The confusion matrix and classification report together show that EcoSort performs
            strongest on materials with <strong>distinctive visual signatures</strong>. For example,
            clear glass containers or rigid cardboard pieces with visible corrugation. These items
            tend to produce confident predictions and high per-class precision and recall.
          </p>

          <p style="text-align: justify;">
            Most errors appear between <strong>visually similar categories</strong>. Thin pieces of
            cardboard may resemble paper, especially when folded or torn, while glossy plastic
            packaging can look similar to metal under strong reflections. The “trash” category is
            naturally diverse and absorbs many ambiguous cases, which lowers its scores relative to
            more uniform classes. These patterns mirror real-world sorting challenges, where even
            people hesitate or disagree on borderline items.
          </p>

          <ul style="margin-top: 10px; line-height: 1.6;">
            <li>
              <strong>Clearly separated classes:</strong> items with unique textures or shapes
              (for example, glass bottles) are recognized most reliably.
            </li>
            <li>
              <strong>Borderline pairs:</strong> confusion is most common between categories that
              share color, thickness, or surface finish (paper vs. cardboard).
            </li>
            <li>
              <strong>Diverse “trash” class:</strong> collects many edge cases and mixed materials,
              which naturally lowers its precision and recall.
            </li>
          </ul>

          <p style="margin-top: 10px; text-align: justify;">
            These class-wise patterns are important because they point to concrete opportunities for
            improvement; such as collecting more examples of borderline items, re-labeling confusing
            samples, or designing clearer signage for categories that people and models both find
            ambiguous.
          </p>
        </div>


                <!-- 4.3 Grad-CAM heatmaps -->
        <div class="card gradcam-card" style="margin-top: 24px;">
          <h3>How EcoSort interprets images (Grad-CAM heatmaps)</h3>

          <p style="text-align: justify;">
            To move beyond raw numbers, EcoSort uses <strong>Grad-CAM</strong> visualizations to see where the
            fine-tuned ResNet-18 model is “looking” when it predicts each class. For a given test image, Grad-CAM
            produces a heatmap that highlights the regions most responsible for the chosen label. Warmer colors indicate
            areas that contributed strongly to the decision.
          </p>

          <p style="text-align: justify;">
            In many correctly classified examples, the heatmaps concentrate on the main object – the body of a bottle,
            the surface of a can, or the texture of a cardboard flap – and largely ignore the background. In harder
            cases, the attention sometimes spreads to shadows or clutter near the object, which helps explain why certain
            items are mis-sorted. These visual checks provide an additional layer of trust: they show that EcoSort’s
            predictions are usually driven by sensible visual cues rather than random noise.
          </p>

          <div class="data-links" style="margin-top: 14px;">
            <a class="link-bubble"
              href="https://github.com/moukthika-gunapaneedu/EcoSort/blob/main/src/gradcam.py"
              target="_blank" rel="noopener noreferrer",
              style="font-weight: 600; border-color: var(--green-soft);">
               View Grad-CAM generation code (gradcam.py)
            </a>
        </div>

          <div class="gradcam-carousel" id="gradcam-carousel">
            <button class="gradcam-nav prev" type="button" aria-label="Previous example">‹</button>
            <button class="gradcam-nav next" type="button" aria-label="Next example">›</button>

            <div class="gradcam-track">
              <!-- Slide 1 -->
              <figure class="gradcam-slide active">
                <img src="results/gradcam/example_0_true-cardboard_pred-cardboard.png"
                    alt="Grad-CAM example 1: true cardboard, predicted cardboard">
                <figcaption>
                 Example 1 – True: <strong>cardboard</strong>, Predicted: <strong>cardboard</strong>.<br>
                  The Grad-CAM shows strong attention along the vertical corner seam and the lower fold of the box.  
                  These regions contain the clearest cardboard texture and depth cues, which the model relies on for its prediction.
                </figcaption>
              </figure>

              <!-- Slide 2 -->
              <figure class="gradcam-slide">
                <img src="results/gradcam/example_1_true-cardboard_pred-cardboard.png"
                    alt="Grad-CAM example 2: true cardboard, predicted cardboard">
                <figcaption>
                  Example 2 – True: <strong>cardboard</strong>, Predicted: <strong>cardboard</strong>.<br>
                  The Grad-CAM highlights the printed barcode area, taped seams, and upper panel of the box. 
                  These regions contain sharp edges, labels, and reflective tape that help the model distinguish 
                  cardboard from smoother materials.
                </figcaption>
              </figure>

              <!-- Slide 3 -->
              <figure class="gradcam-slide">
                <img src="results/gradcam/example_2_true-cardboard_pred-cardboard.png"
                    alt="Grad-CAM example 3: true cardboard, predicted cardboard">
                <figcaption>
                   Example 3 – True: <strong>cardboard</strong>, Predicted: <strong>cardboard</strong>.<br>
                    The Grad-CAM focuses on the horizontal flap cutout and the lower corrugated region of the box. 
                    These areas contain sharp edges, shadows, and consistent cardboard texture, which the model uses 
                    to confirm the material.
                </figcaption>
              </figure>

              <!-- Slide 4 -->
              <figure class="gradcam-slide">
                <img src="results/gradcam/example_3_true-cardboard_pred-cardboard.png"
                    alt="Grad-CAM example 4: true cardboard, predicted cardboard">
                <figcaption>
                  Example 4 – True: <strong>cardboard</strong>, Predicted: <strong>cardboard</strong>.<br>
                  The Grad-CAM concentrates on the center of the torn cardboard piece, including the printed label 
                  and surrounding corrugated texture. These regions provide strong material cues that help the model 
                  distinguish cardboard from the plain background.
                </figcaption>
              </figure>

              <!-- Slide 5 -->
              <figure class="gradcam-slide">
                <img src="results/gradcam/example_4_true-cardboard_pred-cardboard.png"
                    alt="Grad-CAM example 5: true cardboard, predicted cardboard">
                <figcaption>
                  Example 5 – True: <strong>cardboard</strong>, Predicted: <strong>cardboard</strong>.<br>
                  The Grad-CAM focuses on the center of the stacked cardboard piece, especially the exposed corrugated 
                  edges and layered structure. These features provide clear material cues that help the model confirm 
                  the object as cardboard.
                </figcaption>
              </figure>
            </div>

            <div class="gradcam-dots">
              <div class="gradcam-dot active" data-index="0"></div>
              <div class="gradcam-dot" data-index="1"></div>
              <div class="gradcam-dot" data-index="2"></div>
              <div class="gradcam-dot" data-index="3"></div>
              <div class="gradcam-dot" data-index="4"></div>
            </div>
          </div>
        </div>

        <div class="card" style="margin-top: 24px;">
  <h3>What do these results mean?</h3>

  <p style="text-align: justify;">
    Together, these performance metrics and visual explanations show that EcoSort is not only accurate but also
    <strong>behaves in a sensible, interpretable way</strong>. The model reliably identifies materials with distinct
    surface patterns such as cardboard, paper, and glass, while occasional errors arise in categories that overlap
    visually. The Grad-CAM heatmaps confirm that predictions are driven by meaningful regionslike texture, edges, folds,
    and printed labels rather than random background pixels.
  </p>

  <p style="text-align: justify;">
    These outcomes suggest that EcoSort is well-suited for real-world environments where items appear in varied
    orientations, lighting, or levels of wear. A model that consistently attends to the correct object and ignores
    background noise is far more dependable for downstream applications such as smart bins, educational tools,
    or semi-automated sorting workflows.
  </p>

  <p style="text-align: justify; margin-bottom: 0;">
    In short, the results demonstrate that EcoSort is both <strong>high-performing</strong> and 
    <strong>interpretable</strong>. These are the two qualities that make it a strong foundation for expanding to additional 
    materials or more complex real-world scenes.
  </p>
</div>



      </div>
    </section>


<!-- Conclusion -->
<section id="conclusion">
  <div class="section-inner">
    <div class="section-label">Section 5 · Conclusion</div>

    <!-- Shared Image Style -->
    <style>
      .conclusion-img-box {
        width: 100%;
        max-width: 420px;      /* Medium size */
        margin: 20px auto;
        border: 2px solid var(--green-soft);
        border-radius: 14px;
        overflow: hidden;
        padding: 0;
      }
      .conclusion-img-box img {
        width: 100%;
        height: auto;
        display: block;
      }
    </style>


    <div class="card" style="margin-bottom: 28px;">
  <h3>What question did EcoSort try to answer?</h3>

  <div class="conclusion-img-box">
    <img src="website/recycling_factory.png" alt="recycling facility">
  </div>

  <p style="text-align: justify;">
    EcoSort began with a simple guiding question: <strong>everyday images of waste items can help people sort 
    materials more accurately.</strong> Many daily mistakes come from quick judgments made at a bin, and this project 
    set out to understand those visual decisions more closely. By collecting, organizing, and examining hundreds 
    of photos from the TrashNet dataset, EcoSort grounded the exploration in real images that resemble common 
    situations people face. The project focused on the 
    moments when someone must decide where an item belongs. This approach created a direct link 
    between the dataset and real recycling decisions. Each step, from cleaning the images, to standardizing them, and 
    grouping them, served the purpose of studying how objects look when people encounter them. The overall goal 
    was to see whether images alone carry enough information for a system to provide useful sorting guidance. 
    EcoSort uses these observations to better understand where people succeed and where they struggle in waste sorting. 
  </p>
</div>


<div class="card" style="margin-bottom: 28px;">
  <h3>What did studying the images reveal?</h3>

  <div class="conclusion-img-box">
    <img src="website/similar.png" alt="">
  </div>

  <p style="text-align: justify;">
    Reviewing the cleaned dataset revealed important visual patterns that explain why sorting mistakes happen so 
    often. Some materials, such as rigid cardboard boxes or smooth glass bottles, have distinct shapes and textures 
    that make them easy to recognize. Other items, especially thin cardboard, wrinkled paper, or reflective plastic, 
    look surprisingly similar to one another in everyday lighting. These similarities showed that many sorting 
    mistakes are rooted in genuine visual confusion, not carelessness. The process of resizing and standardizing 
    images also highlighted how differently materials can appear depending on shadows, angles, and background 
    contrast. Looking at these variations helped clarify the exact cues like edges, folds, labels, or shine that 
    influence how people interpret each item. The dataset illustrated that sorting is not a simple task, and even 
    well-intentioned people can easily misidentify borderline materials. These insights formed the foundation for 
    understanding how EcoSort’s model would behave.
  </p>
</div>


<div class="card" style="margin-bottom: 28px;">
  <h3>What did the results show?</h3>

  <div class="conclusion-img-box">
    <img src="website/confusion.png" alt="">
  </div>

  <p style="text-align: justify;">
    When EcoSort tested its cleaned and organized dataset on new images, the model correctly identified materials 
    in a large majority of cases. The strongest results appeared in categories with consistent textures and shapes, 
    such as cardboard, paper, and glass. The confusion matrix revealed that most errors happened in predictable 
    places, especially between materials that genuinely look alike in certain conditions. For example, thin 
    cardboard was often near the boundary of the paper category, and some plastic items resembled mixed trash. 
    These patterns reflect real sorting challenges that people face rather than random model mistakes. The results 
    suggest that visual information alone is powerful enough to guide many everyday decisions, as long as the 
    system has been exposed to clean and consistent examples. The analysis also showed that removing corrupted or 
    unclear images improved the model’s reliability. The outcomes essentially demonstrated how consistent preparation 
    and thoughtful dataset cleaning contributed directly to EcoSort’s strong performance.
  </p>
</div>


<div class="card" style="margin-bottom: 28px;">
  <h3>How does this help with real-world sorting?</h3>

  <div class="conclusion-img-box">
    <img src="website/collection.png" alt="">
  </div>

  <p style="text-align: justify;">
    EcoSort’s findings suggest that simple image-based systems could make everyday sorting less confusing. The model 
    tended to focus on the main object in each photo, ignoring backgrounds and distractions, which means it mirrors 
    the way people naturally make quick judgments. Because it performs best on materials that people also find easy, 
    it could support smoother decisions in places where sorting mistakes frequently occur. These results point to 
    opportunities for clearer signage, visual guides, or small digital tools that show people what each category 
    typically looks like. EcoSort demonstrates that even a small, clean dataset can reveal which items cause the 
    most hesitation and why. This information can shape how recycling stations are labeled or how campuses and 
    offices communicate sorting rules. The project highlights the value of providing simple visual cues rather 
    than technical explanations. In this way, EcoSort connects image analysis to everyday behavior in a practical 
    and understandable way.
  </p>
</div>


<div class="card" style="margin-bottom: 28px;">
  <h3>Where could EcoSort go next?</h3>

  <div class="conclusion-img-box">
    <img src="website/future.png" alt="">
  </div>

  <p style="text-align: justify;">
    The project opens several meaningful paths for future exploration. One natural step is to expand the dataset 
    to include compostables, mixed-material items, or photos taken directly at campus bins. These additions would 
    bring the system closer to real-world environments where items rarely appear clean and centered. Another 
    direction would be creating simple tools such as a webpage or mobile interface that allow people to check an 
    item quickly using a photo. EcoSort also provides a foundation for exploring how different communities respond 
    to visual sorting guidance and whether image-based reminders improve daily habits. The project demonstrated 
    that small and thoughtful interventions can make sorting feel more intuitive rather than overwhelming. Future 
    work could focus on identifying where people hesitate most and building targeted examples for those cases. 
    EcoSort shows that clear images and consistent organization can make a meaningful difference in how 
    everyday sorting decisions are made.
  </p>
</div>
  </div>
</section>

<!-- Section 6 · About -->
<section id="about">
  <div class="section-inner">
    <div class="section-label">Section 6 · About</div>

    <div class="card" style="padding: 32px; position: relative;">

      <!-- NAME + TITLE -->
      <h3 style="font-size: 24px; font-weight: 700; margin-bottom: 4px;">
        <strong>Moukthika Gunapaneedu</strong>
      </h3>

      <p style="margin: 0 0 16px 0; font-size: 16px; color: #4b5563;">
        Data Scientist
      </p>

      <div style="
        display: flex;
        gap: 22px;
        margin-bottom: 32px;
      ">

        <!-- LinkedIn -->
        <a href="https://www.linkedin.com/in/moukthika-gunapaneedu" target="_blank">
          <svg height="24" width="24" fill="#15803d" viewBox="0 0 24 24">
            <path d="M4.98 3.5C4.98 4.88 3.86 6 2.5 6S0 4.88 0 3.5 1.12 1 2.5 1 4.98 2.12 4.98 3.5zM.5 8h4V24h-4zM8.5 8h3.8v2.16h.05c.53-.97 1.83-2.16 3.77-2.16 4.03 0 4.78 2.65 4.78 6.1V24h-4v-7.9c0-1.88-.03-4.3-2.62-4.3-2.63 0-3.03 2.05-3.03 4.16V24h-4V8z"/>
          </svg>
        </a>

        <!-- Blogger -->
        <a href="https://mouk-momentum.blogspot.com/" target="_blank">
          <svg width="24" height="24" fill="#15803d" viewBox="0 0 24 24">
            <path d="M14.5 2h-5A7.5 7.5 0 002 9.5v5A7.5 7.5 0 009.5 22h5a7.5 7.5 0 007.5-7.5v-5A7.5 7.5 0 0014.5 2zm-4 5h3a1 1 0 010 2h-3a1 1 0 010-2zm5 10h-5a1 1 0 010-2h5a1 1 0 010 2z"/>
          </svg>
        </a>

        <!-- GitHub -->
        <a href="https://github.com/moukthika-gunapaneedu" target="_blank">
          <svg height="24" width="24" fill="#15803d" viewBox="0 0 24 24">
            <path d="M12 .5a12 12 0 00-3.8 23.4c.6.1.8-.3.8-.6v-2.2c-3.3.7-4-1.6-4-1.6-.6-1.4-1.3-1.8-1.3-1.8-1-.7.1-.7.1-.7 1.2.1 1.9 1.3 1.9 1.3 1 .1.6 2.3-.2 2.9 1.2.1 2.4-.3 3.4-.9-.1-.4-.2-.8-.2-1.3v-2.8c0-.4.1-.7.3-1-2.7-.3-5.5-1.4-5.5-6a4.6 4.6 0 011.2-3.2 4.2 4.2 0 01.1-3.2s1-.3 3.3 1.2a11.3 11.3 0 016 0c2.3-1.5 3.3-1.2 3.3-1.2a4.2 4.2 0 01.1 3.2 4.6 4.6 0 011.2 3.2c0 4.6-2.8 5.7-5.5 6 .3.3.4.6.4 1.1v3.2c0 .3.2.7.8.6A12 12 0 0012 .5z"/>
          </svg>
        </a>

      </div>

      <div style="
        position: absolute;
        top: 78px;
        right: 32px;
        display: flex;
        flex-direction: column;
        gap: 12px;
      ">

        <!-- Portfolio bubble -->
        <a class="link-bubble"
           href="https://moukthika-gunapaneedu.github.io/portfolio-website/"
           target="_blank"
           style="display: flex; align-items: center; gap: 8px;
                  border: 2px solid var(--green-soft);">

          Portfolio
        </a>

        <!-- Email bubble -->
        <a class="link-bubble"
           href="mailto:moukthikagunapaneedu@gmail.com"
           style="display: flex; align-items: center; gap: 8px;
                  border: 2px solid var(--green-soft);">

          moukthikagunapaneedu@gmail.com
        </a>

      </div>

    </div>
  </div>
</section>






  </main>

  <footer>
    EcoSort · AI-assisted waste classification for cleaner, smarter recycling.
  </footer>

   <script>
    document.addEventListener("DOMContentLoaded", function () {
      const carousel = document.getElementById("gradcam-carousel");
      if (!carousel) return;

      const slides = Array.from(carousel.querySelectorAll(".gradcam-slide"));
      const dots   = Array.from(carousel.querySelectorAll(".gradcam-dot"));
      const prev   = carousel.querySelector(".gradcam-nav.prev");
      const next   = carousel.querySelector(".gradcam-nav.next");

      if (!slides.length || !prev || !next) return;

      let current = 0;

      function showSlide(index) {
        current = (index + slides.length) % slides.length;

        slides.forEach((s, i) => s.classList.toggle("active", i === current));
        dots.forEach((d, i) => d.classList.toggle("active", i === current));
      }

      prev.addEventListener("click", () => showSlide(current - 1));
      next.addEventListener("click", () => showSlide(current + 1));

      dots.forEach(dot => {
        dot.addEventListener("click", () => {
          const idx = parseInt(dot.getAttribute("data-index"), 10);
          if (!Number.isNaN(idx)) showSlide(idx);
        });
      });

      // Make sure initial state is correct
      showSlide(0);
    });
  </script>

</body>
</html>
